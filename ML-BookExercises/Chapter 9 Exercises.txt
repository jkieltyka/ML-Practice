Question 1
The main benifits include:
- operations can be run in parallel or across different devices
- Tensorflow can automatically compute gradients for you

Main drawbacks:
- debugging can be harder
- it is harder to learn

Question 2
yes

Question 3
No, the first statement willl compute A and B using the defined graph in two runs while the second statement will do it in one run.
The results will be the same unless the calculation of one variable has side effects on the other calculation.

Question 4
No you can't you would need to merge the graphs to do this.

Question 5
They will use their own copies of the variable if using distributed tensor flow otherwise they will share the variable

Question 6
A variable is created when it's initializer is called. It is destroyed when the session ends.
Whenusing distributed tensorflow, the variables live in containers across sessions so to remove a variable you will need to remove the container.

Question 7
Variables are operations that hold values they are given. Once they have been initialized they can be run to return their value.
Placeholders do not have values and only represent the shape and type of a tensor

Question 8
If a placeholder is not defined at runtime then an exception will be thrown. If the operation does not depend on a placeholder then no exception will be raised.

